{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "\n",
    "# Assume df is your pandas DataFrame and that the target variable is in column 'target'\n",
    "# Features X and target y:\n",
    "df1 = pd.read_csv(\"../datasets/Global_Dataset.csv\", index_col=0)\n",
    "df2 = pd.read_csv(\"../datasets/Single_Dataset.csv\", index_col=0)\n",
    "df3 = pd.read_csv(\"../datasets/GA1628_Dataset.csv\", index_col=0)\n",
    "df4 = pd.read_csv(\"../datasets/GA1824_Dataset.csv\", index_col=0)\n",
    "\n",
    "df1.set_index(\"PREGNANCY-ID-ANON\", inplace=True)\n",
    "df2.set_index(\"PREGNANCY-ID-ANON\", inplace=True)\n",
    "df3.set_index(\"PREGNANCY-ID-ANON\", inplace=True)\n",
    "df4.set_index(\"PREGNANCY-ID-ANON\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_in_ds(df):\n",
    "    # Missing Values in each Column\n",
    "    print(\"Number of NaN in each Column\")\n",
    "    for col_name in df.columns:\n",
    "        print(col_name + \":\" + str(df[col_name].isna().sum()))\n",
    "    for col in df.columns:\n",
    "        print(f\"{col}: {df[col].dtype}\")\n",
    "\n",
    "    return\n",
    "\n",
    "def dataset_info(dataset):\n",
    "\n",
    "    print(f\"Features in Dataset: {list(dataset.columns)}\" )\n",
    "\n",
    "    print(f\"Number if PTB < 37 is 1: {len(dataset.loc[dataset[\"PTB37\"] == 1])}, is 0: {len(dataset.loc[dataset[\"PTB37\"] == 0])}\")\n",
    "    print(\n",
    "        f\"Number if sPTB < 37 is 1: {len(dataset.loc[(dataset['PTB37'] == 1) & (dataset['SpontaneousPTB'] == 1)])}, \" +\n",
    "        f\"is 0: {len(dataset.loc[(dataset['PTB37'] == 0) | (dataset['SpontaneousPTB'] == 0)])}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Number if sPTB < 34 is 1: {len(dataset.loc[(dataset['PTB34'] == 1) & (dataset['SpontaneousPTB'] == 1)])}, \" +\n",
    "        f\"is 0: {len(dataset.loc[(dataset['PTB34'] == 0) | (dataset['SpontaneousPTB'] == 0)])}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Number if sPTB < 32 is 1: {len(dataset.loc[(dataset['PTB32'] == 1) & (dataset['SpontaneousPTB'] == 1)])}, \" +\n",
    "        f\"is 0: {len(dataset.loc[(dataset['PTB32'] == 0) | (dataset['SpontaneousPTB'] == 0)])}\"\n",
    "    )\n",
    "\n",
    "    nan_in_ds(dataset)\n",
    "\n",
    "    return\n",
    "\n",
    "def undersample_50_50(df, target_col='target', random_state=42):\n",
    "    \"\"\"\n",
    "    Returns a new DataFrame with a 50/50 undersample of the given target column.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to undersample.\n",
    "        target_col (str): The column name containing the target variable.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A balanced DataFrame with equal number of samples for each class.\n",
    "    \"\"\"\n",
    "    # Count instances of each class in the target column.\n",
    "    target_counts = df[target_col].value_counts()\n",
    "    \n",
    "    # Identify the minority and majority classes.\n",
    "    minority_class = target_counts.idxmin()\n",
    "    majority_class = target_counts.idxmax()\n",
    "    minority_count = target_counts.min()\n",
    "    \n",
    "    # Split the DataFrame into minority and majority subsets.\n",
    "    df_minority = df[df[target_col] == minority_class]\n",
    "    df_majority = df[df[target_col] == majority_class]\n",
    "    \n",
    "    # Downsample the majority class to the size of the minority class.\n",
    "    df_majority_downsampled = df_majority.sample(n=minority_count, random_state=random_state)\n",
    "    \n",
    "    # Combine the two subsets and shuffle the result.\n",
    "    df_balanced = pd.concat([df_minority, df_majority_downsampled]).sample(frac=1, random_state=random_state)\n",
    "    \n",
    "    return df_balanced\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Datasets and label functions should be defined as in your script\n",
    "datasets = [df1, df2, df3, df4]\n",
    "target_label_defs = {\n",
    "    'PTB37': lambda df: df['PTB37'],\n",
    "    'sPTB37': lambda df: ((df['PTB37'] == 1) & (df['SpontaneousPTB'] == 1)).astype(int),\n",
    "    'sPTB34': lambda df: ((df['PTB34'] == 1) & (df['SpontaneousPTB'] == 1)).astype(int),\n",
    "    'sPTB32': lambda df: ((df['PTB32'] == 1) & (df['SpontaneousPTB'] == 1)).astype(int)\n",
    "}\n",
    "label_cols = ['PTB37', 'PTB34', 'PTB32', 'SpontaneousPTB']\n",
    "\n",
    "# Define models and parameter grids\n",
    "models_param_grids = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [200, 500],\n",
    "            'max_depth': [5, 10, 20],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_features': ['sqrt', 'log2'],\n",
    "            'bootstrap': [True, False]\n",
    "        }\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'model': LogisticRegression(solver='saga', max_iter=2000, random_state=42),\n",
    "        'params': {\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'C': [0.01, 0.1, 1, 10, 100]\n",
    "        }\n",
    "    },\n",
    "    'SVC': {\n",
    "        'model': SVC(probability=True, random_state=42),\n",
    "        'params': {\n",
    "            'C': [0.1],\n",
    "            'kernel': ['linear', 'rbf'],\n",
    "            'gamma': ['auto']\n",
    "        }\n",
    "    },\n",
    "    'XGB': {\n",
    "        'model': XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_jobs=-1, random_state=42, tree_method=\"hist\"),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 500],\n",
    "            'max_depth': [3, 6, 10],\n",
    "            'learning_rate': [0.01, 0.1, 0.3],\n",
    "            'subsample': [0.7, 1],\n",
    "            'colsample_bytree': [0.7, 1]\n",
    "        }\n",
    "    },\n",
    "    'KNN': {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params': {\n",
    "            'n_neighbors': [3, 5, 7, 9],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'p': [1, 2]  # 1=manhattan, 2=euclidean\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "# Custom scoring dictionary for GridSearchCV\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'f1': make_scorer(f1_score),\n",
    "    'roc_auc': make_scorer(roc_auc_score, needs_proba=True)\n",
    "}\n",
    "\n",
    "# Output dictionary to store best results\n",
    "tuning_results = {}\n",
    "\n",
    "for d_idx, df in enumerate(datasets):\n",
    "    ds_name = f\"Dataset_{d_idx+1}\"\n",
    "    tuning_results[ds_name] = {}\n",
    "    feature_cols = [col for col in df.columns if col not in label_cols]\n",
    "    for label_name, label_func in target_label_defs.items():\n",
    "        print(f\"\\n[INFO] {ds_name} - Label: {label_name}\")\n",
    "        df_copy = df.copy()\n",
    "        df_copy['target'] = label_func(df_copy)\n",
    "        df_copy = undersample_50_50(df=df_copy, target_col='target')\n",
    "        X = df_copy[feature_cols]\n",
    "        y = df_copy['target']\n",
    "        tuning_results[ds_name][label_name] = {}\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        for model_name, entry in models_param_grids.items():\n",
    "            print(f\"  Tuning {model_name}...\")\n",
    "            model = entry['model']\n",
    "            param_grid = entry['params']\n",
    "\n",
    "            # Use RandomizedSearch for XGB (faster) and GridSearch for others\n",
    "            if model_name == 'XGB':\n",
    "                search = RandomizedSearchCV(\n",
    "                    estimator=model,\n",
    "                    param_distributions=param_grid,\n",
    "                    n_iter=10,\n",
    "                    scoring='roc_auc',\n",
    "                    cv=cv,\n",
    "                    refit=True,\n",
    "                    verbose=2,\n",
    "                    n_jobs=-1,\n",
    "                    random_state=42\n",
    "                )\n",
    "            else:\n",
    "                search = GridSearchCV(\n",
    "                    estimator=model,\n",
    "                    param_grid=param_grid,\n",
    "                    scoring='roc_auc',\n",
    "                    cv=cv,\n",
    "                    refit=True,\n",
    "                    verbose=2,\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "            search.fit(X, y)\n",
    "\n",
    "            # Evaluate all scoring metrics using cross_val_predict if needed\n",
    "            best_model = search.best_estimator_\n",
    "            cv_results = search.cv_results_\n",
    "            best_params = search.best_params_\n",
    "            best_score = search.best_score_\n",
    "\n",
    "            tuning_results[ds_name][label_name][model_name] = {\n",
    "                'best_params': best_params,\n",
    "                'best_roc_auc': best_score\n",
    "            }\n",
    "            print(f\"    Best ROC AUC: {best_score:.3f} | Params: {best_params}\")\n",
    "\n",
    "# Save results to JSON or CSV\n",
    "import json\n",
    "with open('tuning_results.json', 'w') as f:\n",
    "    json.dump(tuning_results, f, indent=2)\n",
    "\n",
    "print(\"Results saved to tuning_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TESTING SCRIPT FOR SVG, RF, XGBOOST, KNN AND LR MODELS\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, recall_score, precision_score,\n",
    "                             f1_score, roc_auc_score, confusion_matrix)\n",
    "\n",
    "# Load your hyperparameter tuning results\n",
    "with open(\"tuning_results.json\", \"r\") as f:\n",
    "    best_hyperparams = json.load(f)\n",
    "\n",
    "datasets = [df1, df2, df3, df4]\n",
    "target_label_defs = {\n",
    "    'PTB37': lambda df: df['PTB37'],\n",
    "    'sPTB37': lambda df: ((df['PTB37'] == 1) & (df['SpontaneousPTB'] == 1)).astype(int),\n",
    "    'sPTB34': lambda df: ((df['PTB34'] == 1) & (df['SpontaneousPTB'] == 1)).astype(int),\n",
    "    'sPTB32': lambda df: ((df['PTB32'] == 1) & (df['SpontaneousPTB'] == 1)).astype(int)\n",
    "}\n",
    "label_cols = ['PTB37', 'PTB34', 'PTB32', 'SpontaneousPTB']\n",
    "\n",
    "model_classes = {\n",
    "    \"RandomForest\": RandomForestClassifier,\n",
    "    \"LogisticRegression\": LogisticRegression,\n",
    "    \"SVC\": SVC,\n",
    "    \"XGB\": XGBClassifier,\n",
    "    \"KNN\": KNeighborsClassifier,\n",
    "}\n",
    "\n",
    "# Store metrics/results for all models\n",
    "results = {}\n",
    "outer_preds_all = {}\n",
    "cv_fold_metrics = {}\n",
    "\n",
    "for d_idx, df in enumerate(datasets):\n",
    "    ds_name = f\"Dataset_{d_idx+1}\"\n",
    "    results[ds_name] = {}\n",
    "    outer_preds_all[ds_name] = {}\n",
    "    cv_fold_metrics[ds_name] = {}\n",
    "\n",
    "    feature_cols = [col for col in df.columns if col not in label_cols]\n",
    "    for label_name, label_func in target_label_defs.items():\n",
    "        df_copy = df.copy()\n",
    "        df_copy['target'] = label_func(df_copy)\n",
    "        print(f\"Original target counts, dataset #: {d_idx}, label: {label_name}:\")\n",
    "        print(df_copy['target'].value_counts())\n",
    "        df_copy = undersample_50_50(df=df_copy, target_col='target')\n",
    "        X = df_copy[feature_cols]\n",
    "        y = df_copy['target']\n",
    "\n",
    "        cv_fold_metrics[ds_name][label_name] = {}\n",
    "        results[ds_name][label_name] = {}\n",
    "        outer_preds_all[ds_name][label_name] = {}\n",
    "\n",
    "        for model_name in [\"RandomForest\", \"LogisticRegression\", \"SVC\", \"XGB\", \"KNN\"]:\n",
    "            print(f\"  Running {model_name} for {ds_name}, {label_name}\")\n",
    "\n",
    "            # Get best params from json\n",
    "            params = best_hyperparams[ds_name][label_name][model_name]['best_params']\n",
    "            if model_name == \"RandomForest\":\n",
    "                model = RandomForestClassifier(**params, n_jobs=-1, random_state=42)\n",
    "            elif model_name == \"LogisticRegression\":\n",
    "                model = LogisticRegression(**params, solver='saga', max_iter=2000, random_state=42)\n",
    "            elif model_name == \"SVC\":\n",
    "                model = SVC(**params, probability=True, random_state=42, verbose=1,)\n",
    "            elif model_name == \"XGB\":\n",
    "                model = XGBClassifier(**params, use_label_encoder=False, eval_metric='logloss', n_jobs=-1, random_state=42, tree_method='hist')\n",
    "            elif model_name == \"KNN\":\n",
    "                model = KNeighborsClassifier(**params)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown model\")\n",
    "\n",
    "            # Outer CV\n",
    "            outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "            outer_predictions = []\n",
    "            outer_true = []\n",
    "            outer_subject_ids = []\n",
    "            importances_sum = np.zeros(len(feature_cols), dtype=float)\n",
    "            fold_count = 0\n",
    "            fold_accuracies = []\n",
    "            fold_recalls = []\n",
    "            fold_precisions = []\n",
    "            fold_f1s = []\n",
    "            fold_roc_aucs = []\n",
    "            fold_fprs = []\n",
    "            fold_metrics_list = []\n",
    "\n",
    "            for train_idx, test_idx in outer_cv.split(X, y):\n",
    "                X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "                y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "                # Convert to numpy for XGBClassifier\n",
    "                if model_name == \"XGB\":\n",
    "                    X_train_ = X_train.to_numpy()\n",
    "                    X_test_ = X_test.to_numpy()\n",
    "                    y_train_ = y_train.to_numpy()\n",
    "                else:\n",
    "                    X_train_, X_test_, y_train_ = X_train, X_test, y_train\n",
    "\n",
    "                model.fit(X_train_, y_train_)\n",
    "                preds = model.predict_proba(X_test_)[:, 1]\n",
    "                preds_class = (preds >= 0.5).astype(int)\n",
    "\n",
    "                outer_predictions.extend(preds)\n",
    "                outer_true.extend(y_test)\n",
    "                outer_subject_ids.extend(X_test.index)\n",
    "\n",
    "                # Feature importances\n",
    "                if model_name in ['RandomForest', 'XGB']:\n",
    "                    importances_sum += getattr(model, \"feature_importances_\", np.zeros_like(importances_sum))\n",
    "                elif model_name == \"LogisticRegression\":\n",
    "                    # Use absolute value of coefficients, averaged over classes if multiclass\n",
    "                    if len(model.coef_.shape) > 1:\n",
    "                        abs_coefs = np.mean(np.abs(model.coef_), axis=0)\n",
    "                    else:\n",
    "                        abs_coefs = np.abs(model.coef_.flatten())\n",
    "                    importances_sum += abs_coefs\n",
    "\n",
    "                fold_count += 1\n",
    "                acc = accuracy_score(y_test, preds_class)\n",
    "                rec = recall_score(y_test, preds_class)\n",
    "                prec = precision_score(y_test, preds_class, zero_division=0)\n",
    "                f1v = f1_score(y_test, preds_class, zero_division=0)\n",
    "                ra = roc_auc_score(y_test, preds)\n",
    "                tn, fp, fn, tp = confusion_matrix(y_test, preds_class).ravel()\n",
    "                fpr_value = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "                fold_accuracies.append(acc)\n",
    "                fold_recalls.append(rec)\n",
    "                fold_precisions.append(prec)\n",
    "                fold_f1s.append(f1v)\n",
    "                fold_roc_aucs.append(ra)\n",
    "                fold_fprs.append(fpr_value)\n",
    "                fold_metrics_list.append({\n",
    "                    \"accuracy\": acc,\n",
    "                    \"recall\": rec,\n",
    "                    \"precision\": prec,\n",
    "                    \"f1\": f1v,\n",
    "                    \"roc_auc\": ra,\n",
    "                    \"fpr\": fpr_value\n",
    "                })\n",
    "\n",
    "            def mean_std(arr):\n",
    "                return float(np.mean(arr)), float(np.std(arr, ddof=1))\n",
    "            acc_mean, acc_std = mean_std(fold_accuracies)\n",
    "            rec_mean, rec_std = mean_std(fold_recalls)\n",
    "            prec_mean, prec_std = mean_std(fold_precisions)\n",
    "            f1_mean, f1_std = mean_std(fold_f1s)\n",
    "            roc_mean, roc_std = mean_std(fold_roc_aucs)\n",
    "            fpr_mean, fpr_std = mean_std(fold_fprs)\n",
    "\n",
    "            cv_fold_metrics[ds_name][label_name][model_name] = {\n",
    "                \"mean_std\": {\n",
    "                    \"accuracy\": {\"mean\": acc_mean, \"std\": acc_std},\n",
    "                    \"recall\": {\"mean\": rec_mean, \"std\": rec_std},\n",
    "                    \"precision\": {\"mean\": prec_mean, \"std\": prec_std},\n",
    "                    \"f1\": {\"mean\": f1_mean, \"std\": f1_std},\n",
    "                    \"roc_auc\": {\"mean\": roc_mean, \"std\": roc_std},\n",
    "                    \"fpr\": {\"mean\": fpr_mean, \"std\": fpr_std}\n",
    "                },\n",
    "                \"fold_values\": fold_metrics_list\n",
    "            }\n",
    "\n",
    "            if fold_count > 0:\n",
    "                avg_importances = importances_sum / fold_count\n",
    "            else:\n",
    "                avg_importances = np.zeros(len(feature_cols))\n",
    "\n",
    "            # For SVC, set importances to zeros (no importances available)\n",
    "            if model_name == \"SVC\":\n",
    "                avg_importances = np.zeros(len(feature_cols))\n",
    "\n",
    "            outer_predictions = np.array(outer_predictions)\n",
    "            outer_true = np.array(outer_true)\n",
    "            outer_subject_ids = [str(sid) for sid in outer_subject_ids]\n",
    "            outer_preds_all[ds_name][label_name][model_name] = {\n",
    "                \"predictions\": outer_predictions.tolist(),\n",
    "                \"true\": outer_true.tolist(),\n",
    "                \"subject_ids\": outer_subject_ids\n",
    "            }\n",
    "\n",
    "            predicted_classes = (outer_predictions >= 0.5).astype(int)\n",
    "            n_iterations = 1000\n",
    "            n_size = len(outer_true)\n",
    "            metrics_bootstrap = {\n",
    "                'accuracy': [],\n",
    "                'recall': [],\n",
    "                'precision': [],\n",
    "                'f1': [],\n",
    "                'roc_auc': [],\n",
    "                'fpr': []\n",
    "            }\n",
    "\n",
    "            for _ in range(n_iterations):\n",
    "                sample_indices = np.random.choice(np.arange(n_size), size=n_size, replace=True)\n",
    "                sample_true = outer_true[sample_indices]\n",
    "                sample_preds = outer_predictions[sample_indices]\n",
    "                sample_classes = predicted_classes[sample_indices]\n",
    "\n",
    "                acc = accuracy_score(sample_true, sample_classes)\n",
    "                rec = recall_score(sample_true, sample_classes)\n",
    "                prec = precision_score(sample_true, sample_classes, zero_division=0)\n",
    "                f1_val = f1_score(sample_true, sample_classes, zero_division=0)\n",
    "                ra = roc_auc_score(sample_true, sample_preds)\n",
    "                tn, fp, fn, tp = confusion_matrix(sample_true, sample_classes).ravel()\n",
    "                fpr_value = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "                metrics_bootstrap['accuracy'].append(acc)\n",
    "                metrics_bootstrap['recall'].append(rec)\n",
    "                metrics_bootstrap['precision'].append(prec)\n",
    "                metrics_bootstrap['f1'].append(f1_val)\n",
    "                metrics_bootstrap['roc_auc'].append(ra)\n",
    "                metrics_bootstrap['fpr'].append(fpr_value)\n",
    "\n",
    "            metric_summary = {}\n",
    "            for metric in metrics_bootstrap:\n",
    "                vals = np.array(metrics_bootstrap[metric])\n",
    "                mean_val = np.mean(vals)\n",
    "                lower = np.percentile(vals, 2.5)\n",
    "                upper = np.percentile(vals, 97.5)\n",
    "                metric_summary[metric] = {'mean': mean_val, '95CI': (lower, upper)}\n",
    "\n",
    "            feat_import_dict = {\n",
    "                feature_cols[i]: float(avg_importances[i])\n",
    "                for i in range(len(feature_cols))\n",
    "            }\n",
    "            metric_summary[\"feature_importances\"] = feat_import_dict\n",
    "            results[ds_name][label_name][model_name] = metric_summary\n",
    "\n",
    "# Saving results\n",
    "def convert_results_for_json(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_results_for_json(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, (tuple, list)):\n",
    "        return [convert_results_for_json(x) for x in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "with open(\"results.json\", \"w\") as f:\n",
    "    json.dump(convert_results_for_json(results), f, indent=4)\n",
    "print(\"Bootstrapped results saved to results.json\")\n",
    "\n",
    "with open(\"outer_predictions.json\", \"w\") as f:\n",
    "    json.dump(convert_results_for_json(outer_preds_all), f, indent=4)\n",
    "print(\"Outer predictions saved to outer_predictions.json\")\n",
    "\n",
    "with open(\"cv_fold_metrics.json\", \"w\") as f:\n",
    "    json.dump(convert_results_for_json(cv_fold_metrics), f, indent=4)\n",
    "print(\"Fold-level results (mean & std + raw fold data) saved to cv_fold_metrics.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "with open('cv_fold_metrics.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "models = [\"RandomForest\", \"SVC\", \"LogisticRegression\", \"XGB\", \"KNN\"]\n",
    "labels = [\"PTB37\", \"sPTB37\", \"sPTB34\", \"sPTB32\"]\n",
    "datasets = [\"Dataset_1\", \"Dataset_2\", \"Dataset_3\", \"Dataset_4\"]\n",
    "metrics = [\"accuracy\", \"recall\", \"precision\", \"f1\", \"roc_auc\", \"fpr\"]\n",
    "metric_names = {\n",
    "    \"accuracy\": \"Accuracy\",\n",
    "    \"recall\": \"Recall\",\n",
    "    \"precision\": \"Precision\",\n",
    "    \"f1\": \"F1 Score\",\n",
    "    \"roc_auc\": \"ROC AUC\",\n",
    "    \"fpr\": \"False Positive Rate\"\n",
    "}\n",
    "dataset_map = {\n",
    "    \"Dataset_1\": \"General\",\n",
    "    \"Dataset_2\": \"Single\",\n",
    "    \"Dataset_3\": \"GA-16-28\",\n",
    "    \"Dataset_4\": \"GA-18-24\"\n",
    "}\n",
    "\n",
    "def mean_std_str(mean, std):\n",
    "    return f\"{mean:.3f} ± {std:.3f}\"\n",
    "\n",
    "for metric in metrics:\n",
    "    file_name = f\"results_table_{metric}.xlsx\"\n",
    "    with pd.ExcelWriter(file_name) as writer:\n",
    "        for model in models:\n",
    "            table = pd.DataFrame(index=list(dataset_map.values()) + [\"Average Label\"],\n",
    "                                 columns=labels + [\"Average DS\"])\n",
    "            cell_vals = []\n",
    "            # Fill in mean ± std for each cell\n",
    "            for ds_key, ds_name in dataset_map.items():\n",
    "                row_means = []\n",
    "                for label in labels:\n",
    "                    try:\n",
    "                        mean = results[ds_key][label][model]['mean_std'][metric][\"mean\"]\n",
    "                        std = results[ds_key][label][model]['mean_std'][metric][\"std\"]\n",
    "                        table.loc[ds_name, label] = mean_std_str(mean, std)\n",
    "                        row_means.append(mean)\n",
    "                        cell_vals.append(mean)\n",
    "                    except Exception:\n",
    "                        table.loc[ds_name, label] = \"\"\n",
    "                # Average across this dataset's labels\n",
    "                if row_means:\n",
    "                    mean = np.mean(row_means)\n",
    "                    std = np.std(row_means, ddof=1) if len(row_means) > 1 else 0\n",
    "                    table.loc[ds_name, \"Average DS\"] = mean_std_str(mean, std)\n",
    "            # Average per label across all datasets\n",
    "            for label in labels:\n",
    "                col_means = []\n",
    "                for ds_key in dataset_map.keys():\n",
    "                    try:\n",
    "                        mean = results[ds_key][label][model]['mean_std'][metric][\"mean\"]\n",
    "                        col_means.append(mean)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                if col_means:\n",
    "                    mean = np.mean(col_means)\n",
    "                    std = np.std(col_means, ddof=1) if len(col_means) > 1 else 0\n",
    "                    table.loc[\"Average Label\", label] = mean_std_str(mean, std)\n",
    "            # Overall grand mean ± std\n",
    "            if cell_vals:\n",
    "                mean = np.mean(cell_vals)\n",
    "                std = np.std(cell_vals, ddof=1)\n",
    "                table.loc[\"Average Label\", \"Average DS\"] = mean_std_str(mean, std)\n",
    "            # Write to sheet for this model\n",
    "            table.to_excel(writer, sheet_name=model)\n",
    "    print(f\"Excel file '{file_name}' saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load results.json\n",
    "with open(\"results.json\", \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "models = [\"RandomForest\", \"LogisticRegression\"]\n",
    "labels = [\"PTB37\", \"sPTB37\", \"sPTB34\", \"sPTB32\"]\n",
    "datasets = [\"Dataset_1\", \"Dataset_2\", \"Dataset_3\", \"Dataset_4\"]\n",
    "dataset_map = {\n",
    "    \"Dataset_1\": \"General\",\n",
    "    \"Dataset_2\": \"Single\",\n",
    "    \"Dataset_3\": \"GA-16-28\",\n",
    "    \"Dataset_4\": \"GA-18-24\"\n",
    "}\n",
    "metrics = [\"accuracy\", \"recall\", \"precision\", \"f1\", \"roc_auc\", \"fpr\"]\n",
    "metric_names = {\n",
    "    \"accuracy\": \"Accuracy\",\n",
    "    \"recall\": \"Recall\",\n",
    "    \"precision\": \"Precision\",\n",
    "    \"f1\": \"F1 Score\",\n",
    "    \"roc_auc\": \"ROC AUC\",\n",
    "    \"fpr\": \"False Positive Rate\"\n",
    "}\n",
    "\n",
    "def mean_ci_str(metric_data):\n",
    "    try:\n",
    "        mean = metric_data['mean']\n",
    "        ci_low, ci_high = metric_data['95CI']\n",
    "        return f\"{mean:.3f} ({ci_low:.3f}, {ci_high:.3f})\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "for model in models:\n",
    "    file_name = f\"{model}_results.xlsx\"\n",
    "    with pd.ExcelWriter(file_name) as writer:\n",
    "        for label in labels:\n",
    "            table = pd.DataFrame(index=[metric_names[m] for m in metrics],\n",
    "                                 columns=[dataset_map[ds] for ds in datasets])\n",
    "            for ds in datasets:\n",
    "                for m in metrics:\n",
    "                    try:\n",
    "                        metric_data = results[ds][label][model][m]\n",
    "                        table.loc[metric_names[m], dataset_map[ds]] = mean_ci_str(metric_data)\n",
    "                    except Exception:\n",
    "                        table.loc[metric_names[m], dataset_map[ds]] = \"\"\n",
    "            table.to_excel(writer, sheet_name=label)\n",
    "    print(f\"Excel file '{file_name}' saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from scipy.stats import f_oneway, ttest_rel\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Models to analyze\n",
    "MODEL_LIST = [\"RandomForest\", \"LogisticRegression\"]\n",
    "\n",
    "metrics = [\"accuracy\", \"recall\", \"precision\", \"f1\", \"roc_auc\", \"fpr\"]\n",
    "\n",
    "def load_cv_fold_metrics(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def run_anova_and_pairwise(cv_data, model_name):\n",
    "    \"\"\"\n",
    "    For a single model (RF or LR), perform ANOVA and paired t-tests on metrics across datasets.\n",
    "    Returns a nested dictionary: results[label][metric] = {...}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    dataset_names = sorted(cv_data.keys())\n",
    "    # Find all labels present in at least one dataset for this model\n",
    "    all_labels = set()\n",
    "    for ds in dataset_names:\n",
    "        all_labels.update(cv_data[ds].keys())\n",
    "    all_labels = sorted(all_labels)\n",
    "    for label in all_labels:\n",
    "        results[label] = {}\n",
    "        for metric in metrics:\n",
    "            group_data = []\n",
    "            group_names = []\n",
    "            for ds in dataset_names:\n",
    "                # Check if label & model exist for this dataset\n",
    "                if label not in cv_data[ds] or model_name not in cv_data[ds][label]:\n",
    "                    continue\n",
    "                fold_list = cv_data[ds][label][model_name][\"fold_values\"]\n",
    "                fold_vals = [fold_dict[metric] for fold_dict in fold_list if metric in fold_dict]\n",
    "                if len(fold_vals) < 2:  # skip if not enough data\n",
    "                    continue\n",
    "                group_data.append(fold_vals)\n",
    "                group_names.append(ds)\n",
    "            if len(group_data) < 2:\n",
    "                continue\n",
    "            # ANOVA\n",
    "            f_stat, p_anova = f_oneway(*group_data)\n",
    "            results[label][metric] = {\n",
    "                \"anova\": {\"f-stat\": float(f_stat), \"p-value\": float(p_anova)},\n",
    "                \"posthoc\": {}\n",
    "            }\n",
    "            # Posthoc paired t-tests (Bonferroni correction)\n",
    "            if p_anova < 0.05:\n",
    "                idx_pairs = list(combinations(range(len(group_data)), 2))\n",
    "                pvals = []\n",
    "                pair_keys = []\n",
    "                for (i, j) in idx_pairs:\n",
    "                    data_i = group_data[i]\n",
    "                    data_j = group_data[j]\n",
    "                    t_stat, p_val = ttest_rel(data_i, data_j)\n",
    "                    pvals.append(p_val)\n",
    "                    pair_keys.append((group_names[i], group_names[j]))\n",
    "                # Bonferroni correction\n",
    "                reject, pvals_corrected, _, _ = multipletests(pvals, alpha=0.05, method='bonferroni')\n",
    "                for idx, (g1, g2) in enumerate(pair_keys):\n",
    "                    results[label][metric][\"posthoc\"][f\"{g1} vs {g2}\"] = {\n",
    "                        \"p-uncorrected\": float(pvals[idx]),\n",
    "                        \"p-corrected\": float(pvals_corrected[idx]),\n",
    "                        \"significant\": bool(reject[idx])\n",
    "                    }\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    cv_path = \"cv_fold_metrics.json\"\n",
    "    cv_data = load_cv_fold_metrics(cv_path)\n",
    "    for model_name in MODEL_LIST:\n",
    "        print(f\"\\n{model_name}\")\n",
    "        stats_results = run_anova_and_pairwise(cv_data, model_name)\n",
    "        # Print results\n",
    "        for label, metrics_dict in stats_results.items():\n",
    "            print(f\"\\nLabel: {label}\")\n",
    "            for metric, anova_dict in metrics_dict.items():\n",
    "                f_val = anova_dict[\"anova\"][\"f-stat\"]\n",
    "                p_val = anova_dict[\"anova\"][\"p-value\"]\n",
    "                print(f\"  Metric: {metric}\")\n",
    "                print(f\"    ANOVA: F={f_val:.3f}, p={p_val:.3e}\")\n",
    "                if p_val < 0.05:\n",
    "                    for comp, vals in anova_dict[\"posthoc\"].items():\n",
    "                        p_unc = vals[\"p-uncorrected\"]\n",
    "                        p_cor = vals[\"p-corrected\"]\n",
    "                        sig = vals[\"significant\"]\n",
    "                        print(f\"    {comp}: p-unc={p_unc:.3e}, p-cor={p_cor:.3e}, sig={sig}\")\n",
    "        # Save each model's results to its own JSON file\n",
    "        with open(f\"anova_ttest_{model_name}.json\", \"w\") as f:\n",
    "            json.dump(stats_results, f, indent=4)\n",
    "        print(f\"\\nResults saved to anova_ttest_{model_name}.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load results\n",
    "with open('results.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "anova_dict = {\n",
    "    \"RandomForest\": \"anova_ttest_RandomForest.json\",\n",
    "    \"LogisticRegression\": \"anova_ttest_LogisticRegression.json\",\n",
    "}\n",
    "anova_results = {model: json.load(open(fname)) for model, fname in anova_dict.items()}\n",
    "\n",
    "metrics = ['accuracy', 'recall', 'precision', 'f1', 'roc_auc']\n",
    "labels = ['PTB37', 'sPTB37', 'sPTB34', 'sPTB32']\n",
    "datasets = ['Dataset_1', 'Dataset_2', 'Dataset_3', 'Dataset_4']\n",
    "dataset_names = [\"General\", \"Single\", \"GA-16-28\", \"GA-18-24\"]\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n",
    "\n",
    "for model in [\"RandomForest\", \"LogisticRegression\"]:\n",
    "    for lab in labels:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        n_metrics = len(metrics)\n",
    "        n_datasets = len(datasets)\n",
    "        x = np.arange(n_metrics)\n",
    "        width = 0.18\n",
    "\n",
    "        x_positions_dict = {m: {} for m in metrics}\n",
    "        upper_vals_dict = {m: {} for m in metrics}\n",
    "\n",
    "        for i, ds in enumerate(datasets):\n",
    "            means = []\n",
    "            err_low = []\n",
    "            err_high = []\n",
    "            for m in metrics:\n",
    "                data = results[ds][lab][model][m]\n",
    "                mean_val = data['mean']\n",
    "                ci_lower, ci_upper = data['95CI']\n",
    "                means.append(mean_val)\n",
    "                err_low.append(mean_val - ci_lower)\n",
    "                err_high.append(ci_upper - mean_val)\n",
    "\n",
    "                m_idx = metrics.index(m)\n",
    "                x_pos = m_idx - (width * (n_datasets - 1) / 2) + i * width\n",
    "\n",
    "                x_positions_dict[m][ds] = x_pos\n",
    "                upper_vals_dict[m][ds] = ci_upper\n",
    "\n",
    "            x_positions = [x_positions_dict[m][ds] for m in metrics]\n",
    "\n",
    "            ax.errorbar(\n",
    "                x_positions, means, yerr=[err_low, err_high],\n",
    "                fmt='s', capsize=5, color=colors[i],\n",
    "                label=dataset_names[i], markersize=8, linestyle='None'\n",
    "            )\n",
    "\n",
    "        # Cosmetic settings\n",
    "        xlabels = ['Accuracy', 'Recall', 'Precision', 'F1', 'AUC']\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(xlabels)\n",
    "        ax.set_ylabel('Metric Value')\n",
    "        ax.set_title(f'{model} Performance Metrics for {lab}')\n",
    "        ax.legend(labels=dataset_names, loc='lower right')\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "        # Draw significance bars if any\n",
    "        if lab == \"sPTB32\":\n",
    "            anova_file = anova_results[model]\n",
    "            sig_bar_top = {}\n",
    "            for m in metrics:\n",
    "                max_ci = max(upper_vals_dict[m].values())\n",
    "                sig_bar_top[m] = max_ci + 0.02\n",
    "            bar_inset = 0.00\n",
    "            stack_increase = 0.04\n",
    "            vertical_length = 0.02\n",
    "\n",
    "            for m_idx, m in enumerate(metrics):\n",
    "                posthoc = anova_file.get(lab, {}).get(m, {}).get(\"posthoc\", {})\n",
    "                for comp, comp_data in posthoc.items():\n",
    "                    if comp_data.get(\"significant\", False):\n",
    "                        ds1, ds2 = comp.split(\" vs \")\n",
    "                        x1 = x_positions_dict[m][ds1]\n",
    "                        x2 = x_positions_dict[m][ds2]\n",
    "                        if x2 < x1:\n",
    "                            x1, x2 = x2, x1\n",
    "                        y_sig = sig_bar_top[m]\n",
    "                        x1_inset = x1 + bar_inset\n",
    "                        x2_inset = x2 - bar_inset\n",
    "                        y_low = y_sig - vertical_length\n",
    "\n",
    "                        ax.plot([x1_inset, x2_inset], [y_sig, y_sig], color='black', linewidth=1.0)\n",
    "                        ax.plot([x1_inset, x1_inset], [y_low, y_sig], color='black', linewidth=1.0)\n",
    "                        ax.plot([x2_inset, x2_inset], [y_low, y_sig], color='black', linewidth=1.0)\n",
    "                        x_text = (x1_inset + x2_inset) / 2\n",
    "                        ax.text(x_text, y_sig + 0.005, '*', ha='center', va='bottom', fontsize=12)\n",
    "                        sig_bar_top[m] += stack_increase\n",
    "\n",
    "        plt.savefig(f\"box_plot_{lab}_{model}.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
